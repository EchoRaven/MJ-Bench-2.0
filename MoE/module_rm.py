import torch
import json
from swift.llm import (
    get_model_tokenizer, get_template, inference,
    get_default_template_type, get_model_with_value_head
)
from swift.llm.utils.utils import _prepare_inputs
from swift.tuners import Swift
from swift.utils import seed_everything
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from copy import deepcopy

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

import re
import json

import re
import json
from typing import Union, List, Dict

Alignment_prompt = """For VIDEO generated by prompt:"{pt}", determine whether it is good or bad at "{cri}" from the Alignment point of view."""
Bias_prompt = """For VIDEO generated by prompt:"{pt}", determine whether it is good or bad at "{cri}" from the Bias point of view."""
CC_prompt = """For VIDEO generated by prompt:"{pt}", determine whether it is good or bad at "{cri}" from the Coherence and Consistency point of view."""
Quality_prompt = """For VIDEO generated by prompt:"{pt}", determine whether it is good or bad at "{cri}" from the Quality point of view."""
Safety_prompt = """For VIDEO generated by prompt:"{pt}", determine whether it is good or bad at "{cri}" from the Safety point of view."""


def convert_to_json_format(input_string: str) -> Union[dict, List[dict]]:
    # 定义结果容器
    results = []

    # 处理键未加引号的情况，如 `{object:"same", attribute:"first"}` 等
    input_string = re.sub(r'(?<!")(\b\w+\b)(?=\s*:\s*")', r'"\1"', input_string)

    # 如果存在多个 JSON 对象，以换行或其他分隔符分隔的情况
    json_blocks = re.split(r'\n|(?<=\})\s*(?=\{)', input_string)

    for block in json_blocks:
        block = block.strip()  # 去除首尾空白

        # 处理带有前缀的 JSON 片段（如 "VIDEO1 : {...}"）
        match = re.match(r'^\w+\s*:\s*(\{.*\})$', block)
        if match:
            block = match.group(1)

        # 尝试将单引号替换为双引号，以确保 JSON 兼容
        block = block.replace("'", '"')

        # 尝试将字符串转换为 JSON 对象
        try:
            json_obj = json.loads(block)
            results.append(json_obj)
        except json.JSONDecodeError:
            # 忽略无法解析的块
            continue

    # 根据结果数量返回单个字典或列表
    if len(results) == 1:
        return results[0]
    return results


class MJ_VIDEO_RM:
    def __init__(self, config):
        self.config = config
        self.dtype = torch.bfloat16 if config["dtype"] == "bfloat16" else torch.float32
        logging.info("Loading router ...")
        self.router, self.tokenizer = get_model_tokenizer(config["model_type"], self.dtype,
                            model_kwargs={'device_map': 'auto'}, model_id_or_path=config["model_id_or_path"])
        self.router = Swift.from_pretrained(
                    self.router, config["router_path"], "router", inference_mode=True)
        self.router.generation_config.max_new_tokens = 1024
        self.generation_config = deepcopy(getattr(self.router, 'generation_config'))
        self.generation_info = {}
        # define experts
        self.expert_group = {}
        self.expert_keys = []
        for key in config["experts"].keys():
            self.expert_keys.append(key)
            logging.info(f"Loading {key} expert ...")
            self.expert_group[key], _ =  get_model_tokenizer(config["model_type"], self.dtype,
                        model_kwargs={'device_map': 'auto'}, model_id_or_path=config["model_id_or_path"])
            self.expert_group[key] = get_model_with_value_head(self.expert_group[key])
            self.expert_group[key] = Swift.from_pretrained(
                    self.expert_group[key], config["experts"][key], key, inference_mode=True)
            lora_route = config["experts"][key]
            logging.info(f"Loading {key} expert from {lora_route}")
        
        template_type = get_default_template_type(config["model_type"])
        self.template = get_template(template_type, self.tokenizer)
        seed_everything(42)
        logging.info("Loading prompt list ...")
        # load prompt list
        with open(config["prompt_list"], "r", encoding="utf-8") as f:
            self.prompt_list = json.load(f)

    def router_choice(self, video_paths, prompt):
        router_template = self.prompt_list["router"]
        response, _ = inference(self.router, self.template, router_template + prompt, videos=video_paths)
        return response
    
    def activate_expert(self, force_keys, router_response):
        if len(force_keys) > 0:
            return force_keys
        experts = []
        response_result = convert_to_json_format(router_response)
        for key in response_result.keys():
            if response_result[key] == "yes":
               experts.append(key)
        if len(experts) == 0:
            experts = self.expert_keys
        return experts
    
    def process_expert(self, expert, video_paths, prompt):
        criteria = self.prompt_list[expert]
        score_list = {}
        for criterion in criteria:
            if expert == "alignment":
                instruction = Alignment_prompt.format(pt=prompt, cri=criterion)
            elif expert == "bias_fairness":
                instruction = Bias_prompt.format(pt=prompt, cri=criterion)
            elif expert == "coherence_consistency":
                instruction = CC_prompt.format(pt=prompt, cri=criterion)
            elif expert == "safety":
                instruction = Safety_prompt.format(pt=prompt, cri=criterion)
            elif expert == "quality":
                instruction = Quality_prompt.format(pt=prompt, cri=criterion)
            with torch.no_grad():
                inputs, tokenizer_kwargs, token_len, example = _prepare_inputs(self.router, self.template, instruction, videos=video_paths, history=[], generation_config=self.generation_config, generation_info=self.generation_info)
                print(inputs)
                print(instruction)
            # response, _ = inference(self.expert_group[expert], self.template, instruction, videos=video_paths)
            # # score_list[criterion] = response
            # logging.info(f"{criterion} : {response}")
        return score_list, expert
    
    def experts_judge(self, experts, video_paths, prompt):
        def process_expert_concurrently(expert):
            return self.process_expert(expert, video_paths, prompt)
        result = {}
        with ThreadPoolExecutor() as executor:
            future_to_expert = {executor.submit(process_expert_concurrently, expert): expert for expert in experts}
            for future in as_completed(future_to_expert):
                expert = future_to_expert[future]
                try:
                    result_json, _ = future.result()
                    result[expert] = result_json
                except Exception as exc:
                    print(f'{expert} generated an exception: {exc}')
        return result
        
    def judge(self, video_paths, prompt, force_keys=[]):
        if len(force_keys) == 0:
            router_response = self.router_choice(video_paths, prompt)
            logging.info(f"Router : {router_response}")
            experts = self.activate_expert(force_keys, router_response)
        else:
            experts = force_keys
        experts_response = self.experts_judge(experts, video_paths, prompt)
        return experts_response
    
    def inference(self, video_paths, prompt, force_keys=[]):
        pass

if __name__ == "__main__":
    with open("MoE_config_2B_single_reward.json", "r", encoding="utf-8") as f:
        config = json.load(f)
    model = MJ_VIDEO_RM(config)
    force_keys = ["alignment"]
    prompt = "Generate a video that depicts a solitary adventurer wearing a billowing cloak and wielding a sword, positioned in front of an old gate enveloped in a mystical orange glow. The barren scenery featuring remnants and a rock platform enhances the atmosphere of danger and excitement. Include dynamic processes such as the wind blowing the cloak, the sword swinging, and the old gate creaking as it opens. Use cinematic styles such as slow-motion for dramatic effect. End with a close-up of the adventurer's determined face as they step through the gate into the unknown."
    result = model.process_expert("alignment", ["../videos/safesora/56db225a581c1e592d770d0dc2adcd0950734abdc852ce68581a2293f13f2e6f/0e3d08d5cd88366686cbc7c267ea676c941403e00eeb49d8145381bab84bdd91.mp4"], prompt)
    print(result)

